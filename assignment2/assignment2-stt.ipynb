{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76eb3311",
   "metadata": {},
   "source": [
    "# Assignment 2: The \"Smart Labeling Pipeline\" Challenge\n",
    "\n",
    "**Total Marks: 20**\n",
    "\n",
    "Build a cost-effective, high-quality labeling pipeline using human annotation, programmatic rules, and LLMs.\n",
    "\n",
    "This notebook implements an end-to-end smart labeling pipeline to:\n",
    "1. Establish gold standard through human annotation and measure inter-annotator agreement (6 marks)\n",
    "2. Label data programmatically using weak supervision (Snorkel) (6 marks)\n",
    "3. Optimize labeling budget using active learning (5 marks)\n",
    "4. Leverage LLMs for bulk labeling and detect hallucinations (e.g. noisy labels) (3 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03665b6c",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae778e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\projects\\Courses\\sttai\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\ratho\\AppData\\Local\\Temp\\ipykernel_26096\\2465582818.py:13: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  import google.generativeai as genai\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from snorkel.labeling import labeling_function, PandasLFApplier, LFAnalysis\n",
    "from snorkel.labeling.model import LabelModel\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34d483d",
   "metadata": {},
   "source": [
    "## Task 1: The Human as Annotator (6 Marks)\n",
    "\n",
    "**Objective:** Establish a \"Gold Standard\" dataset and measure human consensus.\n",
    "\n",
    "### Part 1.1: Parse Annotator CSV Files\n",
    "\n",
    "After annotating the first 100 reviews, export annotations from three annotators (A, B, C) as CSV files.\n",
    "Parse these CSV files into clean DataFrames for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "290b8d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotator_csv(csv_path):\n",
    "    \"\"\"\n",
    "    Parses annotator CSV file into a clean DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        csv_path (str): Path to annotator CSV file\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with columns ['review_id', 'review', 'label']\n",
    "                     where label is one of: 'Positive', 'Negative', 'Neutral'\n",
    "    \n",
    "    Note:\n",
    "        - Look for relevant column names in the CSV file\n",
    "        - If column names differ, the function will try to map them appropriately\n",
    "        - Finally, return with two columns 'review' and 'label'\n",
    "    \"\"\"\n",
    "    # TODO: Load CSV file using pd.read_csv()\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # TODO: Check and map column names if needed\n",
    "    df.columns = [col.strip().lower() for col in df.columns] # lower the column name\n",
    "    if 'review_id' not in df.columns:\n",
    "        df['review_id'] = df.index  # create id = index if review_id column is not present\n",
    "    if 'review' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain a 'review' column\")\n",
    "    if 'sentiment' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain a 'sentiment' column\")\n",
    "    \n",
    "    return df[['review_id', 'review', 'sentiment']].rename(columns={'sentiment': 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81905fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Annotator A ====================\n",
      "   review_id                                             review     label\n",
      "0          0  This movie is a triumph in every sense. Highly...  Positive\n",
      "1          1  I have never been so bored in my life. The sco...  Negative\n",
      "2          2  I was completely blown away by this film. The ...  Positive\n",
      "3          3  The trailer was better than the movie. The act...  Negative\n",
      "4          4  Middle of the road entertainment. Visually it'...   Neutral\n",
      "==================== Annotator B ====================\n",
      "   review_id                                             review     label\n",
      "0          0  This movie is a triumph in every sense. Highly...  Positive\n",
      "1          1  I have never been so bored in my life. The sco...  Negative\n",
      "2          2  I was completely blown away by this film. The ...   Neutral\n",
      "3          3  The trailer was better than the movie. The act...  Negative\n",
      "4          4  Middle of the road entertainment. Visually it'...   Neutral\n",
      "==================== Annotator C ====================\n",
      "   review_id                                             review     label\n",
      "0          0  This movie is a triumph in every sense. Highly...  Positive\n",
      "1          1  I have never been so bored in my life. The sco...  Negative\n",
      "2          2  I was completely blown away by this film. The ...  Positive\n",
      "3          3  The trailer was better than the movie. The act...  Negative\n",
      "4          4  Middle of the road entertainment. Visually it'...   Neutral\n"
     ]
    }
   ],
   "source": [
    "# TODO: Parse CSV files (replace with actual file paths)\n",
    "df_a = parse_annotator_csv('./annotator_a.csv')\n",
    "df_b = parse_annotator_csv('./annotator_b.csv')\n",
    "df_c = parse_annotator_csv('./annotator_c.csv')\n",
    "# Display sample data\n",
    "print(\"=\"*20, \"Annotator A\", \"=\"*20)\n",
    "print(df_a.head())\n",
    "print(\"=\"*20, \"Annotator B\", \"=\"*20)\n",
    "print(df_b.head())\n",
    "print(\"=\"*20, \"Annotator C\", \"=\"*20)\n",
    "print(df_c.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdcf20f",
   "metadata": {},
   "source": [
    "### Part 1.2: Implement Fleiss' Kappa from Scratch\n",
    "\n",
    "Measure inter-annotator agreement using Fleiss' Kappa statistic.\n",
    "Implement the formula from scratch and compare with statsmodels implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "886fd3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fleiss_kappa_scratch(rating_matrix):\n",
    "    \"\"\"\n",
    "    Computes Fleiss' Kappa for multiple raters from scratch.\n",
    "    \n",
    "    Args:\n",
    "        rating_matrix (np.array): A Count Matrix of shape (N, k).\n",
    "                                  - N = number of items (rows)\n",
    "                                  - k = number of categories (columns)\n",
    "                                  - Element [i, j] = Count of raters who assigned category j to item i.\n",
    "                                  Example: \n",
    "                                    [[0, 0, 3],   # Item 0: All 3 raters said Category 2\n",
    "                                     [1, 2, 0]]   # Item 1: 1 rater said Cat 0, 2 said Cat 1\n",
    "                            \n",
    "    \n",
    "    Returns:\n",
    "        float: Kappa score (ranges from -1 to 1, where 1 = perfect agreement)\n",
    "    \n",
    "    Formula:\n",
    "        κ = (P_bar - P_e_bar) / (1 - P_e_bar)\n",
    "        \n",
    "        where:\n",
    "        - P_bar = (1/N) * Σ(P_i) = average proportion of agreement across all items\n",
    "        - P_i = (1/(n*(n-1))) * Σ(k_ij * (k_ij - 1)) for item i\n",
    "        - P_e_bar = Σ(p_j^2) = expected agreement by chance\n",
    "        - p_j = proportion of all assignments to category j\n",
    "    \n",
    "    Note:\n",
    "        - N = number of items (samples)\n",
    "        - n = number of raters per item (should be constant)\n",
    "        - k_ij = number of raters who assigned category j to item i\n",
    "    \"\"\"\n",
    "    # TODO: Calculate P_bar (observed agreement), P_e_bar (expected agreement by chance), Apply the formula: κ = (P_bar - P_e_bar) / (1 - P_e_bar)\n",
    "    n = np.sum(rating_matrix[0]) # number of rater per item\n",
    "    p_i = (1/(n*(n-1))) * np.sum(rating_matrix * (rating_matrix - 1), axis=1) # p_i for each item\n",
    "    P_bar = np.mean(p_i) # Observed agreement\n",
    "    p_j = np.sum(rating_matrix, axis=0) / (np.sum(rating_matrix)) # proportion of all assignments\n",
    "    P_e_bar = np.sum(p_j ** 2) # Expected agreement\n",
    "    kappa = (P_bar - P_e_bar) / (1 - P_e_bar) # Fleiss's Kappa\n",
    "    return kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bb440bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fleiss' Kappa (Scratch): 0.784453263993431\n",
      "Fleiss' Kappa (Statsmodels): 0.784453263993431\n",
      "Difference between the two implementations: 0.0\n"
     ]
    }
   ],
   "source": [
    "def prepare_rating_matrix(df_a, df_b, df_c):\n",
    "    \"\"\"\n",
    "    Converts three DataFrames into a rating matrix for Fleiss' Kappa calculation.\n",
    "    \n",
    "    Args:\n",
    "        df_a, df_b, df_c: DataFrames with columns ['review_id', 'review', 'label']\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Rating matrix of shape (N_samples, N_categories)\n",
    "                  where categories are ['Negative', 'Neutral', 'Positive']\n",
    "    \"\"\"\n",
    "    # TODO: Merge the three DataFrames on review\n",
    "    # Hint: Use pd.merge() or pd.concat() with proper keys\n",
    "    df_merged = (df_a.merge(\n",
    "        df_b, on=['review_id', 'review'], suffixes=('_a', '_b'))\n",
    "        .merge(df_c, on=['review_id', 'review'])\n",
    "        .rename(columns={'label': 'label_c'})\n",
    "    ) # merge array on review_id and review column and rename label columns\n",
    "    \n",
    "    # TODO: Return numpy array of shape (N_samples, 3)\n",
    "    # Order: [Negative_count, Neutral_count, Positive_count] for each row\n",
    "    category_mapping = {'Negative': 0, 'Neutral': 2, 'Positive': 1} # catgory mapping\n",
    "    rating_matrix = np.zeros((df_merged.shape[0], 3), dtype=int) # initialize rating matrix\n",
    "    for idx, row in df_merged.iterrows():\n",
    "        labels = [row['label_a'], row['label_b'], row['label_c']] # extract labels from the row\n",
    "        for label in labels:\n",
    "            rating_matrix[idx, category_mapping[label]] += 1\n",
    "            \n",
    "    return rating_matrix\n",
    "\n",
    "# TODO: Prepare rating matrix and calculate Fleiss' Kappa\n",
    "rating_matrix = prepare_rating_matrix(df_a, df_b, df_c) # prepare rating matrix\n",
    "kappa_scratch = fleiss_kappa_scratch(rating_matrix) # calculate kappa from scratch\n",
    "# TODO: Use statsmodels to calculate Fleiss' Kappa\n",
    "kappa_statsmodels = fleiss_kappa(rating_matrix) # calculate kappa using statsmodels\n",
    "# TODO: Print the difference between the two implementations\n",
    "print(\"Fleiss' Kappa (Scratch):\", kappa_scratch)\n",
    "print(\"Fleiss' Kappa (Statsmodels):\", kappa_statsmodels)\n",
    "print(\"Difference between the two implementations:\", kappa_scratch - kappa_statsmodels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9407cec1",
   "metadata": {},
   "source": [
    "### Part 1.3: Conflict Resolution\n",
    "\n",
    "Identify conflicts where annotators disagree and resolve them using majority vote.\n",
    "For complete ties (all three differ), default to 'Neutral'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5aed962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_conflicts(df_a, df_b, df_c):\n",
    "    \"\"\"\n",
    "    Merges annotations from 3 annotators, resolves disagreements using Majority Vote,\n",
    "    and handles complete ties by defaulting to 'Neutral'.\n",
    "    \n",
    "    Args:\n",
    "        df_a, df_b, df_c: DataFrames from each annotator with columns ['review', 'label']\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Final DataFrame with resolved labels (gold standard)\n",
    "                     Columns: ['review', 'label']\n",
    "    \n",
    "    Logic:\n",
    "        - Majority Vote: If 2 annotators agree, use their label\n",
    "        - Tie-Breaker: If all 3 differ (e.g., Positive vs. Negative vs. Neutral), assign 'Neutral'\n",
    "    \"\"\"\n",
    "    df_merged = (df_a.merge(\n",
    "        df_b, on=['review_id', 'review'], suffixes=('_a', '_b'))\n",
    "        .merge(df_c, on=['review_id', 'review'])\n",
    "        .rename(columns={'label': 'label_c'})\n",
    "    )\n",
    "    \n",
    "    def majority_vote(row):\n",
    "        labels = [row['label_a'], row['label_b'], row['label_c']]\n",
    "        if labels.count(labels[0]) >= 2:  # If at least 2 annotators agree\n",
    "            return labels[0]\n",
    "        elif labels.count(labels[1]) >= 2:\n",
    "            return labels[1]\n",
    "        elif labels.count(labels[2]) >= 2:\n",
    "            return labels[2]\n",
    "        else:\n",
    "            return 'Neutral'  # If all three differ assign Neutral\n",
    "        \n",
    "    df_merged['resolved_label'] = df_merged.apply(majority_vote, axis=1) # apply majority vote to resolve conflicts\n",
    "    conflicts = df_merged[(df_merged[['label_a', 'label_b', 'label_c']].nunique(axis=1) > 2)] # identify conflicts where all labels are different\n",
    "    return df_merged[['review', 'resolved_label']].rename(columns={'resolved_label': 'label'}), conflicts # return df with columns review and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4eec0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Conflicts ====================\n",
      "Total Conflicts: 0\n",
      "Empty DataFrame\n",
      "Columns: [review_id, review, label_a, label_b, label_c, resolved_label]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# TODO: Resolve conflicts and create gold standard\n",
    "df_gold_standard, conflicts = resolve_conflicts(df_a, df_b, df_c)\n",
    "# TODO: Display 5 examples of conflicting reviews (if <5 reviews, show all)\n",
    "# Show what A, B, and C each said, and the final resolved label\n",
    "print(\"=\"*20, \"Conflicts\", \"=\"*20)\n",
    "print(\"Total Conflicts:\", conflicts.shape[0])\n",
    "print(conflicts.head(5))\n",
    "\n",
    "# TODO: Save gold standard to CSV\n",
    "df_gold_standard.to_csv('gold_standard_100.csv', index=False) # save gold standard to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2645b7",
   "metadata": {},
   "source": [
    "## Task 2: Weak Supervision (The \"Lazy\" Labeler) (6 Marks)\n",
    "\n",
    "**Objective:** Label the next 200 reviews programmatically to save time.\n",
    "\n",
    "### Part 2.1: Heuristic Development\n",
    "\n",
    "Analyze patterns in the gold standard and write at least 3 heuristic functions.\n",
    "Apply them to the remaining 200 unlabeled reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e06eca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for labeling functions\n",
    "POSITIVE = 1\n",
    "NEGATIVE = 0\n",
    "NEUTRAL = 2\n",
    "ABSTAIN = -1\n",
    "\n",
    "# TODO: Load gold standard to analyze patterns\n",
    "df_gold_standard = pd.read_csv('./gold_standard_100.csv') # load gold standard csv\n",
    "\n",
    "# TODO: Analyze patterns (e.g., common positive/negative words, review length, etc.)\n",
    "# This will help you design effective heuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0681ea",
   "metadata": {},
   "source": [
    "### Part 2.2: Snorkel Labeling Functions\n",
    "\n",
    "Wrap your heuristics as Snorkel @labeling_function decorators.\n",
    "Each function should return POSITIVE (1), NEGATIVE (0), NEUTRAL (2), or ABSTAIN (-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6a3bfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def lf_keyword_great(x):\n",
    "    \"\"\"\n",
    "    Example labeling function: Check if \"great\" appears in the review.\n",
    "    Returns POSITIVE if found, otherwise ABSTAIN.\n",
    "    \"\"\"\n",
    "    # TODO: Check if \"great\" (case-insensitive) is in x.review\n",
    "    # Return POSITIVE if found, ABSTAIN otherwise\n",
    "    return POSITIVE if \"great\" in x.review.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_short_review(x):\n",
    "    \"\"\"\n",
    "    Label based on review length.\n",
    "    Very short reviews might be neutral or indicate lack of engagement.\n",
    "    \"\"\"\n",
    "    # TODO: Implement logic based on review length\n",
    "    # Return appropriate label (NEUTRAL for very short, or ABSTAIN)\n",
    "    return NEUTRAL if len(x.review) < 20 else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_regex_bad(x):\n",
    "    \"\"\"\n",
    "    Use regex to find negative patterns.\n",
    "    Look for words like \"horrible\", \"terrible\", \"awful\", etc.\n",
    "    \"\"\"\n",
    "    # TODO: Use regex or string matching to find negative keywords\n",
    "    # Return NEGATIVE if found, ABSTAIN otherwise\n",
    "    words = [\"horrible\", \"terrible\", \"awful\", \"worst\", \"zero\", \"garbage\", \"waste\", \"disappointing\", \"avoid\", \"misfire\"]\n",
    "    return NEGATIVE if any(word in x.review.lower() for word in words) else ABSTAIN\n",
    "\n",
    "# TODO: Write at least 3 more labeling functions (minimum 6 total)\n",
    "@labeling_function()\n",
    "def lf_regex_positive(x):\n",
    "    \"\"\"\n",
    "    Use regex to find positive patterns.\n",
    "    Look for words like \"amazing\", \"fantastic\", \"love\", etc.\n",
    "    \"\"\"\n",
    "    words = [\"triumph\", \"perfect\", \"masterpiece\", \"hooked\"]\n",
    "    return POSITIVE if any(word in x.review.lower() for word in words) else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_keyword_wow(x):\n",
    "    \"\"\"\n",
    "    Check if \"wow\" appears in the review.\n",
    "    Returns POSITIVE if found, otherwise ABSTAIN.\n",
    "    \"\"\"\n",
    "    return POSITIVE if \"wow\" in x.review.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_keyword_bored(x):\n",
    "    \"\"\"\n",
    "    Check if \"bored\" appears in the review.\n",
    "    Returns NEGATIVE if found, otherwise ABSTAIN.\n",
    "    \"\"\"\n",
    "    return NEGATIVE if \"bored\" in x.review.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_keyword_fine(x):\n",
    "    \"\"\"\n",
    "    Check if \"fine\" appears in the review.\n",
    "    Returns NEUTRAL if found, otherwise ABSTAIN.\n",
    "    \"\"\"\n",
    "    return NEUTRAL if \"fine\" in x.review.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def lf_regex_neutral(x):\n",
    "    \"\"\"\n",
    "    Use regex to find neutral patterns.\n",
    "    Look for words like \"fine\", \"confused\", etc.\n",
    "    \"\"\"\n",
    "    words = [\"confused\", \"oscillated\", \"forgettable\", \"average\", \"categorize\", \"strange\", \"mixed\"]\n",
    "    return NEUTRAL if any(word in x.review.lower() for word in words) else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab62dc4",
   "metadata": {},
   "source": [
    "### Part 2.3: Apply Labeling Functions and Analyze Coverage\n",
    "\n",
    "Apply all labeling functions to the 200 unlabeled reviews and calculate coverage and conflict rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e11384ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/220 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [00:00<00:00, 2975.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LF: LabelingFunction lf_keyword_great, Preprocessors: [], Coverage: 5.00%, Conflicts: 3.18%\n",
      "LF: LabelingFunction lf_short_review, Preprocessors: [], Coverage: 0.00%, Conflicts: 3.18%\n",
      "LF: LabelingFunction lf_regex_bad, Preprocessors: [], Coverage: 16.36%, Conflicts: 3.18%\n",
      "LF: LabelingFunction lf_keyword_wow, Preprocessors: [], Coverage: 3.18%, Conflicts: 3.18%\n",
      "LF: LabelingFunction lf_keyword_bored, Preprocessors: [], Coverage: 3.18%, Conflicts: 3.18%\n",
      "LF: LabelingFunction lf_regex_positive, Preprocessors: [], Coverage: 11.82%, Conflicts: 3.18%\n",
      "LF: LabelingFunction lf_keyword_fine, Preprocessors: [], Coverage: 10.45%, Conflicts: 3.18%\n",
      "LF: LabelingFunction lf_regex_neutral, Preprocessors: [], Coverage: 18.64%, Conflicts: 3.18%\n",
      "                   j Polarity  Coverage  Overlaps  Conflicts\n",
      "lf_keyword_great   0      [1]  0.050000  0.009091   0.009091\n",
      "lf_short_review    1       []  0.000000  0.000000   0.000000\n",
      "lf_regex_bad       2      [0]  0.163636  0.013636   0.004545\n",
      "lf_keyword_wow     3      [1]  0.031818  0.004545   0.000000\n",
      "lf_keyword_bored   4      [0]  0.031818  0.022727   0.013636\n",
      "lf_regex_positive  5      [1]  0.118182  0.013636   0.009091\n",
      "lf_keyword_fine    6      [2]  0.104545  0.031818   0.009091\n",
      "lf_regex_neutral   7      [2]  0.186364  0.040909   0.018182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\projects\\Courses\\sttai\\.venv\\Lib\\site-packages\\snorkel\\labeling\\analysis.py:61: FutureWarning: Input has data type int64, but the output has been cast to float64.  In the future, the output data type will match the input. To avoid this warning, set the `dtype` parameter to `None` to have the output dtype match the input, or set it to the desired output data type.\n",
      "  m = sparse.diags(np.ravel(self._L_sparse.max(axis=1).todense()))\n"
     ]
    }
   ],
   "source": [
    "def analyze_weak_labels(L_matrix, lfs):\n",
    "    \"\"\"\n",
    "    Prints Coverage and Conflict statistics for the Labeling Functions.\n",
    "    \n",
    "    Args:\n",
    "        L_matrix (np.array): Label matrix of shape (N_samples, N_functions)\n",
    "                            Each column represents one labeling function's outputs\n",
    "                            Values: POSITIVE (1), NEGATIVE (0), NEUTRAL (2), ABSTAIN (-1)\n",
    "        lfs: List of labeling functions (for display names)\n",
    "    \n",
    "    Metrics to calculate:\n",
    "        - Coverage: Percentage of non-abstain votes per LF\n",
    "        - Conflict Rate: Percentage of samples where LFs disagree\n",
    "    \"\"\"\n",
    "    # TODO: Calculate coverage for each labeling function\n",
    "    # Coverage = (number of non-abstain votes) / (total samples) * 100\n",
    "    coverage = []\n",
    "    for i in range(L_matrix.shape[1]):\n",
    "        non_abstain_count = np.sum(L_matrix[:, i] != ABSTAIN)\n",
    "        coverage.append((non_abstain_count / L_matrix.shape[0]) * 100)\n",
    "    \n",
    "    # TODO: Calculate conflict rate\n",
    "    # Conflict occurs when multiple LFs label the same sample differently\n",
    "    # Conflict Rate = (number of conflicting samples) / (total samples) * 100\n",
    "    conflict_count = 0\n",
    "    for i in range(L_matrix.shape[0]):\n",
    "        unique_labels = set(L_matrix[i, :])\n",
    "        if ABSTAIN in unique_labels:\n",
    "            unique_labels.remove(ABSTAIN)\n",
    "        if len(unique_labels) > 1:\n",
    "            conflict_count += 1\n",
    "    conflict_rate = (conflict_count / L_matrix.shape[0]) * 100\n",
    "    \n",
    "    # TODO: Print statistics in a readable format\n",
    "    # Hint: Use LFAnalysis from snorkel for detailed stats (optional)\n",
    "    # Or print manually: LF name, Coverage %, Conflicts count\n",
    "    for i, lf in enumerate(lfs):\n",
    "        print(f\"LF: {lf}, Coverage: {coverage[i]:.2f}%, Conflicts: {conflict_rate:.2f}%\")\n",
    "\n",
    "# TODO: Load the 200 unlabeled reviews (you can load the entire dataset and then filter as per the requirement)\n",
    "df_unlabeled = pd.read_csv('Movie_review - unique_movie_reviews.csv').loc[100:]\n",
    "\n",
    "# TODO: Apply all labeling functions to create L_matrix\n",
    "lfs = [lf_keyword_great, lf_short_review, lf_regex_bad, lf_keyword_wow, lf_keyword_bored, lf_regex_positive, lf_keyword_fine, lf_regex_neutral]  # Add all your LFs\n",
    "applier = PandasLFApplier(lfs=lfs) # create a pandas lf applier\n",
    "L_matrix = applier.apply(df=df_unlabeled) # create L_matrix\n",
    "\n",
    "# TODO: Analyze c\n",
    "# overage and conflicts\n",
    "analyze_weak_labels(L_matrix, lfs)\n",
    "\n",
    "# TODO: Use LFAnalysis for detailed statistics\n",
    "analysis = LFAnalysis(L=L_matrix, lfs=lfs)\n",
    "print(analysis.lf_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552be82e",
   "metadata": {},
   "source": [
    "### Part 2.4: Majority Vote Adjudication\n",
    "\n",
    "Use majority vote to generate probabilistic labels (weak labels) for the 200 reviews.\n",
    "Save the result to `weak_labels_200.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "523e974f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing O...\n",
      "INFO:root:Estimating \\mu...\n",
      "  0%|          | 0/500 [00:00<?, ?epoch/s]INFO:root:[0 epochs]: TRAIN:[loss=0.051]\n",
      "INFO:root:[100 epochs]: TRAIN:[loss=0.003]\n",
      " 20%|██        | 102/500 [00:00<00:00, 995.65epoch/s]INFO:root:[200 epochs]: TRAIN:[loss=0.002]\n",
      " 48%|████▊     | 238/500 [00:00<00:00, 1182.76epoch/s]INFO:root:[300 epochs]: TRAIN:[loss=0.002]\n",
      " 72%|███████▏  | 362/500 [00:00<00:00, 1197.33epoch/s]INFO:root:[400 epochs]: TRAIN:[loss=0.001]\n",
      "100%|██████████| 500/500 [00:00<00:00, 1174.36epoch/s]\n",
      "INFO:root:Finished Training\n"
     ]
    }
   ],
   "source": [
    "# TODO: Train LabelModel to get probabilistic labels\n",
    "label_model = LabelModel(cardinality=3, verbose=True) # create label model with 3 classes\n",
    "label_model.fit(L_train=L_matrix, n_epochs=500, log_freq=100, seed=42)\n",
    "\n",
    "# TODO: Convert numeric labels to match your label scheme\n",
    "# Label mapping: 0 -> 'Negative' (or 0), 1 -> 'Positive' (or 1), 2 -> 'Neutral' (or 2), -1 -> 'Abstain'\n",
    "probabilistic_labels = label_model.predict_proba(L=L_matrix) # get probabilistic labels\n",
    "numeric_labels = label_model.predict(L=L_matrix) # get numeric labels\n",
    "label_mapping = {0: 'Negative', 1: 'Positive', 2: 'Neutral', -1: 'Abstain'} # label mapping\n",
    "string_labels = [label_mapping[num] for num in numeric_labels] # convert numeric labels to string labels\n",
    "\n",
    "# TODO: Create DataFrame with reviews and weak labels\n",
    "df_weak_labels = df_unlabeled.copy()\n",
    "df_weak_labels['weak_label'] = string_labels\n",
    "\n",
    "# TODO: Save to CSV\n",
    "df_weak_labels.to_csv('weak_labels_200.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2087d6",
   "metadata": {},
   "source": [
    "## Task 3: Active Learning (The Budget Optimizer) (5 Marks)\n",
    "\n",
    "**Objective:** Simulate cost savings by training a model iteratively.\n",
    "\n",
    "### Part 3.1: Query Strategy Implementation\n",
    "\n",
    "Implement Least Confidence and Entropy Sampling from scratch.\n",
    "These strategies select the most informative samples for labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a236fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_confidence_sampling(model, X_pool, n_instances=10):\n",
    "    \"\"\"\n",
    "    Selects samples where the model is least confident (uncertainty sampling).\n",
    "    \n",
    "    Args:\n",
    "        model: Trained classifier with predict_proba() method\n",
    "        X_pool: Feature matrix of unlabeled samples\n",
    "        n_instances: Number of samples to select\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Indices of selected samples\n",
    "        \n",
    "    Strategy:\n",
    "        Uncertainty = 1 - max(probability) across all classes\n",
    "        For 3-class classification: Get probabilities for [Negative, Positive, Neutral]\n",
    "        Select samples with highest uncertainty (lowest max probability)\n",
    "    \"\"\"\n",
    "    # TODO: Get probability predictions from model\n",
    "\n",
    "    \n",
    "    # TODO: Calculate uncertainty: 1 - max(probability) for each sample\n",
    "\n",
    "    \n",
    "    # TODO: Select top n_instances samples with highest uncertainty\n",
    "    \n",
    "    pass\n",
    "\n",
    "def entropy_sampling(model, X_pool, n_instances=10):\n",
    "    \"\"\"\n",
    "    Selects samples with highest entropy (information gain).\n",
    "    \n",
    "    Args:\n",
    "        model: Trained classifier with predict_proba() method\n",
    "        X_pool: Feature matrix of unlabeled samples\n",
    "        n_instances: Number of samples to select\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Indices of selected samples\n",
    "        \n",
    "    Strategy:\n",
    "        Entropy = -sum(p * log(p)) for all classes\n",
    "        For 3-class classification: Calculate entropy across [Negative, Positive, Neutral] probabilities\n",
    "        Select samples with highest entropy (most uncertain across all classes)\n",
    "    \"\"\"\n",
    "    # TODO: Get probability predictions from model\n",
    "    \n",
    "    # TODO: Calculate entropy: -sum(p * log(p)) for each sample\n",
    "    # Add small epsilon (1e-9) to avoid log(0) errors\n",
    "    \n",
    "    # TODO: Select top n_instances samples with highest entropy\n",
    "    \n",
    "    pass\n",
    "\n",
    "def random_sampling(model, X_pool, n_instances=10):\n",
    "    \"\"\"\n",
    "    Baseline strategy: Selects random samples.\n",
    "    \n",
    "    Args:\n",
    "        model: Not used, but kept for interface consistency\n",
    "        X_pool: Feature matrix of unlabeled samples\n",
    "        n_instances: Number of samples to select\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Randomly selected indices\n",
    "    \"\"\"\n",
    "    # TODO: Randomly select n_instances indices from X_pool\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1af4c33",
   "metadata": {},
   "source": [
    "### Part 3.2: Data Processing and Setup\n",
    "\n",
    "Load the gold standard (seed) and weak labels (pool).\n",
    "Create a static test set from the pool for evaluation.\n",
    "Vectorize text data using TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e76e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data():\n",
    "    \"\"\"\n",
    "    Loads and processes data for active learning.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple: (X_seed, y_seed, X_pool, y_pool, X_test, y_test, vectorizer)\n",
    "               All X are feature matrices, all y are label arrays\n",
    "               vectorizer is returned for later use on LLM data\n",
    "               \n",
    "    Note:\n",
    "        - Seed: gold_standard_100.csv (100 labeled reviews)\n",
    "        - Pool: weak_labels_200.csv (200 reviews, labels treated as hidden for simulation)\n",
    "        - Test: Hold out 50 samples from pool (weak labels) for static evaluation\n",
    "        - We use 3-class classification: Positive (1), Negative (0), Neutral (2)\n",
    "        - Uncertainty metrics use probability scores across all three classes:\n",
    "          * Least Confidence: 1 - max(probabilities) across all classes\n",
    "          * Entropy: -sum(p * log(p)) for all three classes\n",
    "    \"\"\"\n",
    "\n",
    "    df_seed = pd.read_csv('gold_standard_100.csv')\n",
    "    df_pool_full = pd.read_csv('weak_labels_200.csv')\n",
    "    \n",
    "    # Ensure both have 'review' column\n",
    "    if 'review' not in df_seed.columns:\n",
    "        raise ValueError(\"gold_standard_100.csv must have 'review' column\")\n",
    "    if 'review' not in df_pool_full.columns:\n",
    "        raise ValueError(\"weak_labels_200.csv must have 'review' column\")\n",
    "    \n",
    "    # Handle both 'label' and 'sentiment' column names\n",
    "    label_col_seed = 'label' if 'label' in df_seed.columns else 'sentiment'\n",
    "    label_col_pool = 'label' if 'label' in df_pool_full.columns else 'sentiment'\n",
    "    \n",
    "    # Map text labels to numeric: Positive=1, Negative=0, Neutral=2\n",
    "    label_mapping = {\n",
    "        'Positive': 1, 'positive': 1, 'POSITIVE': 1,\n",
    "        'Negative': 0, 'negative': 0, 'NEGATIVE': 0,\n",
    "        'Neutral': 2, 'neutral': 2, 'NEUTRAL': 2\n",
    "    }\n",
    "    \n",
    "    # Convert seed labels\n",
    "    if df_seed[label_col_seed].dtype == 'object':\n",
    "        df_seed['sentiment_numeric'] = df_seed[label_col_seed].map(label_mapping)\n",
    "        if df_seed['sentiment_numeric'].isna().any():\n",
    "            raise ValueError(f\"Unknown labels in seed data: {df_seed[df_seed['sentiment_numeric'].isna()][label_col_seed].unique()}\")\n",
    "    else:\n",
    "        df_seed['sentiment_numeric'] = df_seed[label_col_seed].values\n",
    "    \n",
    "    # Convert pool labels\n",
    "    if df_pool_full[label_col_pool].dtype == 'object':\n",
    "        df_pool_full['sentiment_numeric'] = df_pool_full[label_col_pool].map(label_mapping)\n",
    "        if df_pool_full['sentiment_numeric'].isna().any():\n",
    "            raise ValueError(f\"Unknown labels in pool data: {df_pool_full[df_pool_full['sentiment_numeric'].isna()][label_col_pool].unique()}\")\n",
    "    else:\n",
    "        df_pool_full['sentiment_numeric'] = df_pool_full[label_col_pool].values\n",
    "    \n",
    "    # Create static test set (hold out 50 samples from pool)\n",
    "    df_pool, df_test = train_test_split(df_pool_full, test_size=50, random_state=42)\n",
    "    \n",
    "    # Vectorize text data using TfidfVectorizer\n",
    "    # Fit vectorizer on ALL text (seed + pool + test) to ensure consistent dimensions\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "    all_text = pd.concat([df_seed['review'], df_pool['review'], df_test['review']])\n",
    "    vectorizer.fit(all_text)\n",
    "    \n",
    "    # Transform datasets to feature matrices\n",
    "    X_seed = vectorizer.transform(df_seed['review']).toarray()\n",
    "    X_pool = vectorizer.transform(df_pool['review']).toarray()\n",
    "    X_test = vectorizer.transform(df_test['review']).toarray()\n",
    "    \n",
    "    # Extract numeric labels\n",
    "    y_seed = df_seed['sentiment_numeric'].values\n",
    "    y_pool = df_pool['sentiment_numeric'].values\n",
    "    y_test = df_test['sentiment_numeric'].values\n",
    "    \n",
    "    # Return all datasets and vectorizer\n",
    "    return X_seed, y_seed, X_pool, y_pool, X_test, y_test, vectorizer\n",
    "\n",
    "# TODO: uncomment below codes, to use these variables further\n",
    "# X_seed, y_seed, X_pool, y_pool, X_test, y_test, vectorizer = load_and_process_data()\n",
    "\n",
    "# print(f\"Seed Size: {len(y_seed)}\")\n",
    "# print(f\"Pool Size: {len(y_pool)} (Available for querying)\")\n",
    "# print(f\"Test Size: {len(y_test)} (Held out for evaluation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd86c256",
   "metadata": {},
   "source": [
    "### Part 3.3: Active Learning Loop\n",
    "\n",
    "Implement the iterative active learning loop:\n",
    "1. Train model on current training set\n",
    "2. Query uncertain samples from pool\n",
    "3. \"Label\" them (reveal ground truth)\n",
    "4. Add to training set and retrain\n",
    "5. Log test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbc766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_active_learning_loop(X_seed, y_seed, X_pool, y_pool, X_test, y_test, \n",
    "                             strategy_func, steps=5, batch_size=10):\n",
    "    \"\"\"\n",
    "    Simulates the active learning loop (matches lab approach).\n",
    "    \n",
    "    Args:\n",
    "        X_seed, y_seed: Initial training data (seed set)\n",
    "        X_pool, y_pool: Unlabeled pool (y_pool is hidden, revealed during query)\n",
    "        X_test, y_test: Static test set for evaluation\n",
    "        strategy_func: Function that selects samples (e.g., least_confidence_sampling)\n",
    "                      Signature: strategy_func(model, X_pool, n_instances) -> indices\n",
    "        steps: Number of iterations\n",
    "        batch_size: Number of samples to query per iteration\n",
    "    \n",
    "    Returns:\n",
    "        Tuple: (n_labels_history, accuracy_history)\n",
    "               Lists tracking number of labels and test accuracy over iterations\n",
    "    \"\"\"\n",
    "    # TODO: Initialize training set with seed data\n",
    "\n",
    "    \n",
    "    # TODO: Create working copies of pool (we'll remove samples as we query them)\n",
    "\n",
    "    \n",
    "    # TODO: Initialize empty lists to track progress (accuracy_history, n_labels_history)\n",
    "\n",
    "    \n",
    "    # Train initial model on seed data\n",
    "    model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # TODO: Evaluate initial model and log results\n",
    "\n",
    "    \n",
    "    # TODO: Iterative loop (repeat 'steps' times):\n",
    "    #   for i in range(steps):\n",
    "    #       1. Query: Use strategy_func(model, X_pool_curr, batch_size) to get indices\n",
    "    #       2. \"Label\": Reveal ground truth: y_new = y_pool_curr[query_indices]\n",
    "    #       3. Add to training set: use np.vstack() to add new samples\n",
    "    #       4. Remove from pool: use np.delete() to remove queried samples\n",
    "    #       5. Retrain model: use model.fit() to update the model\n",
    "    #       6. Evaluate on test set, get accuracy\n",
    "    #       7. Log: accuracy_history.append(acc), n_labels_history.append(len(y_train))\n",
    "    \n",
    "    # TODO: Return history lists\n",
    "    pass\n",
    "\n",
    "# TODO: Run active learning with least confidence strategy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c2408d",
   "metadata": {},
   "source": [
    "### Part 3.4: Visualization and Comparison\n",
    "\n",
    "Plot learning curves comparing Active Learning vs. Random Sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cc60750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run active learning with random sampling (baseline)\n",
    "# TODO: Plot learning curves of active learning and random sampling wrt to number of samples\n",
    "\n",
    "\n",
    "# TODO: Print comparison summary for active learning and random sampling final accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3633624",
   "metadata": {},
   "source": [
    "## Task 4: AI vs. AI (LLM & Noise Detection) (3 Marks)\n",
    "\n",
    "**Objective:** Use LLMs for bulk labeling and detect hallucinations.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "- Make an account at [open-router](https://openrouter.ai/) and get the API key.\n",
    "- Use `google/gemini-2.5-flash-lite` (free tier) model as your LLM. Read the documentation on how to use it [here](https://openrouter.ai/google/gemini-2.5-flash-lite/api)\n",
    "- Set environment variable using .env file and paste your API key in it.\n",
    "\n",
    "### Part 4.1: LLM Pipeline with Few-Shot Prompting\n",
    "\n",
    "Design a few-shot prompt with 3 examples from gold standard.\n",
    "Send remaining unlabeled samples (~150) to Gemini API for labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d313c225",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv('OPENROUTER_API_KEY')\n",
    "SITE_URL = \"http://localhost:8000\"  #for OpenRouter rankings\n",
    "SITE_NAME = \"Student Lab Assignment\"\n",
    "\n",
    "MODEL_NAME = \"google/gemini-2.5-flash-lite\"\n",
    "\n",
    "if not API_KEY:\n",
    "    print(\"⚠ Warning: OPENROUTER_API_KEY not found. Please check your .env file.\")\n",
    "\n",
    "\n",
    "def generate_few_shot_prompt(review_text, examples):\n",
    "    \"\"\"\n",
    "    Constructs a few-shot prompt with 3 gold examples + target review.\n",
    "    \n",
    "    Args:\n",
    "        review_text (str): The review to be labeled\n",
    "        examples (list): List of 3 example dictionaries with 'review' and 'label' keys\n",
    "    \n",
    "    Returns:\n",
    "        str: Formatted prompt string\n",
    "    \"\"\"\n",
    "    \n",
    "    pass\n",
    "\n",
    "def query_openrouter(review_text, examples):\n",
    "    \"\"\"\n",
    "    Sends request to OpenRouter API with retry logic and parsing.\n",
    "    \n",
    "    Args:\n",
    "        review_text (str): Review to classify\n",
    "        examples (list): Few-shot examples (list of dicts with 'review' and 'label')\n",
    "    \n",
    "    Returns:\n",
    "        str: Label ('Positive', 'Negative', or 'Neutral')\n",
    "             Returns None if API fails or response is invalid\n",
    "    \n",
    "    Note:\n",
    "        - Uses OpenRouter API endpoint: https://openrouter.ai/api/v1/chat/completions\n",
    "        - Implements retry logic for rate limit errors (429)\n",
    "        - Parses response from OpenRouter's chat completions format\n",
    "    \"\"\"\n",
    "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "    \n",
    "    # TODO: Set up headers:\n",
    "    \n",
    "    # TODO: Generate prompt using generate_few_shot_prompt()\n",
    "    \n",
    "    # TODO: Create payload dictionary:\n",
    "    \n",
    "    # TODO: Implement retry logic:\n",
    "    \n",
    "    # TODO: Parse successful response:\n",
    "    \n",
    "    pass\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "\n",
    "# TODO: Load gold standard examples for few-shot prompting\n",
    "\n",
    "# TODO: Load remaining unlabeled reviews (~150, select last 150 from movie_reviews_300.csv)\n",
    "\n",
    "\n",
    "# TODO: Query OpenRouter for each review\n",
    "# Handle free tier requests per minute (RPM) limit of ~15\n",
    "\n",
    "# TODO: Save LLM labels, in csv format with 'review' and 'label' columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bca488",
   "metadata": {},
   "source": [
    "### Part 4.2: Noise Hunting (Cleanlab Logic)\n",
    "\n",
    "Train a Logistic Regression model on LLM-labeled data.\n",
    "Identify \"High Confidence Disagreements\" where the model is very confident (>0.80) but disagrees with the LLM label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100fd6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_label_errors(llm_labels, model_probs, review_texts, threshold=0.90):\n",
    "    \"\"\"\n",
    "    Detects high-confidence disagreements between model predictions and LLM labels.\n",
    "    This implements Cleanlab logic: find cases where model is confident but disagrees with LLM.\n",
    "    \n",
    "    Args:\n",
    "        llm_labels: List/array of labels from Gemini (numeric: 0=Negative, 1=Positive, 2=Neutral)\n",
    "        model_probs: Probability matrix from Logistic Regression (shape: N_samples, N_classes)\n",
    "        review_texts: List of review texts (for display)\n",
    "        threshold: Confidence threshold (default 0.90)\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dictionaries with suspicious review information\n",
    "              Each dict contains: 'index', 'text', 'llm_label', 'model_pred', 'confidence'\n",
    "    \"\"\"\n",
    "    # TODO: Get model predictions from probabilities\n",
    "\n",
    "    \n",
    "    # TODO: Get model confidence (max probability) for each sample\n",
    "\n",
    "    \n",
    "    # TODO: Convert llm_labels to numeric if they are strings\n",
    "    # Map 'Positive'->1, 'Negative'->0, 'Neutral'->2\n",
    "    \n",
    "    # TODO: Find disagreements where:\n",
    "    #   Hint: disagreement_mask = (preds != llm_labels) & (confidences > threshold)\n",
    "    \n",
    "    # TODO: Create list of suspicious reviews with all relevant information (llm label, model prediction, confidence)\n",
    "\n",
    "    \n",
    "    # TODO: Sort by confidence (highest first) to find most egregious errors\n",
    "    \n",
    "    # TODO: Return list of suspicious reviews\n",
    "    pass\n",
    "\n",
    "\n",
    "# TODO: Load LLM labels in dataframe\n",
    "\n",
    "# TODO: Vectorize LLM-labeled reviews (use same vectorizer from Task 3)\n",
    "\n",
    "# TODO: Train Logistic Regression on LLM-labeled data\n",
    "# Use same model configuration as Task 3 for consistency\n",
    "\n",
    "# TODO: Get probabilities on the same data (self-check), shape should be (N_samples, N_classes)\n",
    "\n",
    "# TODO: Find label errors using your function\n",
    "\n",
    "# TODO: Print top 5 suspicious reviews (if <5, print all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c560870",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "\n",
    "**Submission Checklist:**\n",
    "- [ ] Completed Jupyter Notebook with all tasks (Tasks 1-4)\n",
    "- [ ] Include your label-studio annotation interface screenshot.\n",
    "- [ ] gold_standard_100.csv\n",
    "- [ ] weak_labels_200.csv\n",
    "- [ ] llm_labels_150.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
